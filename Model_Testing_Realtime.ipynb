{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import librosa\n",
        "\n",
        "import librosa.display\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "from tensorflow.keras.layers import Input, Dense, RNN, Layer\n",
        "\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "from tensorflow.keras.regularizers import l2\n",
        "\n",
        "# tf.keras.backend.clear_session()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "from tensorflow.keras.layers import Input, Dense, RNN, Layer\n",
        "\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "from tensorflow.keras.regularizers import l2\n",
        "\n",
        "# tf.keras.backend.clear_session()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {},
      "outputs": [],
      "source": [
        "def clipped_relu(x):\n",
        "\n",
        "    return tf.keras.activations.relu(x, max_value=20.0)\n",
        "\n",
        "\n",
        "\n",
        "def clipped_sigmoid(x):\n",
        "\n",
        "    return tf.keras.activations.sigmoid(x) * 10.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {},
      "outputs": [],
      "source": [
        "import tensorflow.keras as keras\n",
        "@keras.utils.register_keras_serializable(package=\"Custom\", name=\"LTCCell\")\n",
        "class LTCCell(Layer):\n",
        "\n",
        "    def __init__(self, units, ode_unfolds=6, l2_reg=0.001, **kwargs):\n",
        "\n",
        "        super(LTCCell, self).__init__(**kwargs)\n",
        "\n",
        "        self.units = units\n",
        "\n",
        "        self.ode_unfolds = ode_unfolds\n",
        "\n",
        "        self.state_size = units\n",
        "\n",
        "        self.l2_reg = l2_reg  # L2 regularization parameter\n",
        "\n",
        "\n",
        "\n",
        "    def build(self, input_shape):\n",
        "\n",
        "        self.input_dim = input_shape[-1]\n",
        "\n",
        "\n",
        "\n",
        "        # Trainable parameters with L2 regularization\n",
        "\n",
        "        self.W = self.add_weight(\n",
        "\n",
        "            shape=(self.input_dim + self.units, self.units),\n",
        "\n",
        "            initializer='glorot_uniform',\n",
        "\n",
        "            regularizer=l2(self.l2_reg),\n",
        "\n",
        "            name='W'\n",
        "\n",
        "        )\n",
        "\n",
        "        self.bias = self.add_weight(\n",
        "\n",
        "            shape=(self.units,),\n",
        "\n",
        "            initializer='zeros',\n",
        "\n",
        "            name='bias'\n",
        "\n",
        "        )\n",
        "\n",
        "        self.tau = self.add_weight(\n",
        "\n",
        "            shape=(self.units,),\n",
        "\n",
        "            initializer='ones',\n",
        "\n",
        "            name='tau'\n",
        "\n",
        "        )\n",
        "\n",
        "\n",
        "\n",
        "        # Additional parameters for the LTC model\n",
        "\n",
        "        self.C = self.add_weight(\n",
        "\n",
        "            shape=(self.units,),\n",
        "\n",
        "            initializer='ones',\n",
        "\n",
        "            name='C'\n",
        "\n",
        "        )\n",
        "\n",
        "        self.G = self.add_weight(\n",
        "\n",
        "            shape=(self.units,),\n",
        "\n",
        "            initializer='ones',\n",
        "\n",
        "            name='G'\n",
        "\n",
        "        )\n",
        "\n",
        "\n",
        "\n",
        "        super(LTCCell, self).build(input_shape)\n",
        "\n",
        "\n",
        "\n",
        "    def call(self, inputs, states):\n",
        "\n",
        "        prev_state = states[0]\n",
        "\n",
        "        concatenated = tf.concat([inputs, prev_state], axis=1)\n",
        "\n",
        "        dt = 0.01  # Time step\n",
        "\n",
        "\n",
        "\n",
        "        for _ in range(self.ode_unfolds):\n",
        "\n",
        "            dh = (-prev_state + tf.nn.tanh(tf.matmul(concatenated, self.W) + self.bias)) / tf.nn.softplus(self.tau)\n",
        "\n",
        "            prev_state += dt * dh\n",
        "\n",
        "\n",
        "\n",
        "        # Apply custom activation functions\n",
        "\n",
        "        prev_state = clipped_relu(prev_state)\n",
        "\n",
        "        prev_state = prev_state * clipped_sigmoid(self.C)\n",
        "\n",
        "        prev_state = prev_state / clipped_sigmoid(self.G)\n",
        "\n",
        "\n",
        "\n",
        "        return prev_state, [prev_state]\n",
        "    \n",
        "    def get_config(self):\n",
        "        \n",
        "        config = super(LTCCell, self).get_config()\n",
        "        \n",
        "        config.update({\"units\": self.units})\n",
        "        \n",
        "        return config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pyaudio\n",
        "import numpy as np\n",
        "import librosa\n",
        "import tensorflow as tf \n",
        "import tensorflow.keras as keras\n",
        "\n",
        "# Load your model\n",
        "model = keras.models.load_model('emotion_model_63.h5', custom_objects={\"LTCCell\": LTCCell})\n",
        "\n",
        "# Preprocessing function \n",
        "def extract_features_from_audio(audio, sample_rate):\n",
        "    mfcc = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=13)\n",
        "    mfcc_delta = librosa.feature.delta(mfcc)\n",
        "    mfcc_delta2 = librosa.feature.delta(mfcc, order=2)\n",
        "    features = np.concatenate((mfcc, mfcc_delta, mfcc_delta2), axis=0)\n",
        "    features = features.T  # Shape: (time_steps, features)\n",
        "\n",
        "    # Expected shape for the model\n",
        "    expected_time_steps = 911\n",
        "    expected_features = 39\n",
        "\n",
        "    # Check if the number of time steps matches the expected shape\n",
        "    if features.shape[0] < expected_time_steps:\n",
        "        # Pad with zeros\n",
        "        pad_width = ((0, expected_time_steps - features.shape[0]), (0, 0))  # Correct format\n",
        "        features = np.pad(features, pad_width, mode='constant')\n",
        "    else:\n",
        "        # Truncate\n",
        "        features = features[:expected_time_steps, :]\n",
        "\n",
        "    return features\n",
        "\n",
        "# Audio stream setup\n",
        "def start_audio_stream():\n",
        "    p = pyaudio.PyAudio()\n",
        "    stream = p.open(format=pyaudio.paFloat32,\n",
        "                    channels=1,\n",
        "                    rate=16000, \n",
        "                    input=True,\n",
        "                    frames_per_buffer=1024)\n",
        "    return p, stream\n",
        "\n",
        "# Real-time emotion detection\n",
        "def real_time_emotion_detection():\n",
        "    # Start audio stream\n",
        "    p, stream = start_audio_stream()\n",
        "    print(\"Listening for real-time audio...\")\n",
        "\n",
        "    # Buffer to store audio chunks\n",
        "    audio_buffer = []\n",
        "    buffer_size = 3 * 16000  # 3 seconds of audio at 16kHz\n",
        "\n",
        "    # Sliding window to store the last 3 sessions (9 seconds total)\n",
        "    sliding_window = []\n",
        "    max_sessions = 2\n",
        "\n",
        "    try:\n",
        "        while True:\n",
        "            # Capture audio\n",
        "            audio_data = stream.read(1024, exception_on_overflow=False)\n",
        "            audio_data = np.frombuffer(audio_data, dtype=np.float32)\n",
        "\n",
        "            # Append to buffer\n",
        "            audio_buffer.extend(audio_data)\n",
        "\n",
        "            # If buffer has enough data for 5 seconds\n",
        "            if len(audio_buffer) >= buffer_size:\n",
        "                # Extract the 5-second audio segment\n",
        "                audio_segment = np.array(audio_buffer[:buffer_size])\n",
        "\n",
        "                # Add the current segment to the sliding window\n",
        "                sliding_window.append(audio_segment)\n",
        "\n",
        "                # Ensure the sliding window contains only the last 3 sessions\n",
        "                if len(sliding_window) > max_sessions:\n",
        "                    sliding_window.pop(0)  # Remove the oldest session\n",
        "\n",
        "                # If we have 3 sessions (15 seconds of audio), process them\n",
        "                if len(sliding_window) == max_sessions:\n",
        "                    # Concatenate the last 3 sessions (15 seconds of audio)\n",
        "                    combined_audio = np.concatenate(sliding_window)\n",
        "\n",
        "                    # Extract features from the combined audio\n",
        "                    features = extract_features_from_audio(combined_audio, sample_rate=16000)\n",
        "\n",
        "                    # Reshape features to match the model's input shape\n",
        "                    features = np.expand_dims(features, axis=0)  # Add batch dimension\n",
        "\n",
        "                    # Predict emotion\n",
        "                    prediction = model.predict(features, verbose=0)\n",
        "                    emotion_index = np.argmax(prediction, axis=1)[0]\n",
        "\n",
        "                    # Map index to emotion label (customize this based on your model's labels)\n",
        "                    emotion_labels = [\"angry\", \"happy\", \"sad\", \"neutral\"]  # Replace with your labels\n",
        "                    detected_emotion = emotion_labels[emotion_index]\n",
        "\n",
        "                    # Display result\n",
        "                    print(f\"Detected Emotion: {detected_emotion}\")\n",
        "\n",
        "                # Clear buffer for the next segment\n",
        "                audio_buffer = audio_buffer[buffer_size:]\n",
        "\n",
        "    except KeyboardInterrupt:\n",
        "        print(\"Stopped listening.\")\n",
        "    finally:\n",
        "        # Clean up\n",
        "        stream.stop_stream()\n",
        "        stream.close()\n",
        "        p.terminate()\n",
        "\n",
        "# Main function\n",
        "if __name__ == \"__main__\":\n",
        "    real_time_emotion_detection()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "dl",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
